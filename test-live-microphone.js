#!/usr/bin/env node

/**
 * Live Microphone Speech-to-Text Test Script
 * 
 * This script captures audio from your microphone and transcribes it in real-time
 * using the Sherpa ONNX + Moonshine setup.
 * 
 * Usage: node test-live-microphone.js
 * 
 * Instructions:
 * 1. Run the script
 * 2. When prompted, speak into your microphone
 * 3. Press Ctrl+C to stop and see the transcription
 */

const fs = require('fs');
const path = require('path');

// Configuration
const PLUGIN_DIR = '/Users/skkoripe/Library/CloudStorage/GoogleDrive-sairamk0@gmail.com/My Drive/obsidian-files/Second brain/.obsidian/plugins/obsiscribe';
const NATIVE_LIB_DIR = path.join(PLUGIN_DIR, 'native-modules', 'sherpa-onnx-darwin-arm64');
const NATIVE_MODULE_PATH = path.join(NATIVE_LIB_DIR, 'sherpa-onnx.node');
const MODEL_DIR = path.join(PLUGIN_DIR, 'sherpa-onnx-moonshine-tiny-en-int8');

console.log('üé§ Live Microphone Speech-to-Text Test');
console.log('======================================');
console.log();

let sherpa = null;
let recognizer = null;
let stream = null;
let isRecording = false;

async function loadNativeModule() {
    console.log('üì¶ Loading native module...');
    
    // Set up library path
    const originalPath = process.env.DYLD_LIBRARY_PATH || '';
    process.env.DYLD_LIBRARY_PATH = NATIVE_LIB_DIR + (originalPath ? ':' + originalPath : '');

    try {
        sherpa = require(NATIVE_MODULE_PATH);
        console.log('‚úÖ Native module loaded successfully!');
        return sherpa;
    } catch (error) {
        console.error('‚ùå Failed to load native module:', error.message);
        process.exit(1);
    } finally {
        // Restore original library path
        if (originalPath) {
            process.env.DYLD_LIBRARY_PATH = originalPath;
        } else {
            delete process.env.DYLD_LIBRARY_PATH;
        }
    }
}

async function setupRecognizer() {
    console.log('‚öôÔ∏è  Setting up speech recognizer...');

    // Configuration for Moonshine model
    const config = {
        'featConfig': {
            'sampleRate': 16000,
            'featureDim': 80,
        },
        'modelConfig': {
            'moonshine': {
                'preprocessor': path.join(MODEL_DIR, 'preprocess.onnx'),
                'encoder': path.join(MODEL_DIR, 'encode.int8.onnx'),
                'uncachedDecoder': path.join(MODEL_DIR, 'uncached_decode.int8.onnx'),
                'cachedDecoder': path.join(MODEL_DIR, 'cached_decode.int8.onnx'),
            },
            'tokens': path.join(MODEL_DIR, 'tokens.txt'),
            'numThreads': 2,
            'provider': 'cpu',
            'debug': 0, // Disable debug for cleaner output
        }
    };

    try {
        // Create recognizer and stream
        recognizer = sherpa.createOfflineRecognizer(config);
        stream = sherpa.createOfflineStream(recognizer);
        
        console.log('‚úÖ Speech recognizer ready!');
        return true;
    } catch (error) {
        console.error('‚ùå Failed to setup recognizer:', error.message);
        return false;
    }
}

// Simple audio recording using Node.js (requires additional setup)
// For now, we'll simulate the microphone input workflow
async function simulateMicrophoneInput() {
    console.log('üéôÔ∏è  Simulating microphone input...');
    console.log();
    console.log('üìù INSTRUCTIONS:');
    console.log('   This script demonstrates the transcription workflow.');
    console.log('   In a real implementation, this would capture live audio from your microphone.');
    console.log();
    
    // Generate speech-like audio (more complex than sine wave)
    console.log('üéµ Generating speech-like test audio...');
    
    const sampleRate = 16000;
    const duration = 3; // 3 seconds
    const numSamples = sampleRate * duration;
    const audioSamples = new Float32Array(numSamples);
    
    // Generate more speech-like audio with multiple frequencies and variations
    for (let i = 0; i < numSamples; i++) {
        const t = i / sampleRate;
        
        // Mix multiple frequencies to simulate speech formants
        const f1 = 200 + 100 * Math.sin(2 * Math.PI * 2 * t); // Varying fundamental
        const f2 = 800 + 200 * Math.sin(2 * Math.PI * 3 * t); // First formant
        const f3 = 2400 + 400 * Math.sin(2 * Math.PI * 1.5 * t); // Second formant
        
        // Amplitude modulation to simulate speech envelope
        const envelope = 0.5 * (1 + Math.sin(2 * Math.PI * 4 * t)) * Math.exp(-t * 0.5);
        
        audioSamples[i] = envelope * (
            0.4 * Math.sin(2 * Math.PI * f1 * t) +
            0.3 * Math.sin(2 * Math.PI * f2 * t) +
            0.2 * Math.sin(2 * Math.PI * f3 * t) +
            0.1 * (Math.random() - 0.5) // Add some noise
        );
    }
    
    console.log(`‚úÖ Generated ${numSamples} speech-like audio samples`);
    
    return {
        samples: audioSamples,
        sampleRate: sampleRate,
        duration: duration
    };
}

async function transcribeAudio(audioData) {
    console.log('üîÑ Transcribing audio...');
    
    try {
        // Accept waveform using the correct API
        sherpa.acceptWaveformOffline(stream, {
            sampleRate: audioData.sampleRate,
            samples: audioData.samples
        });
        console.log('‚úÖ Audio data accepted');
        
        // Decode the audio
        sherpa.decodeOfflineStream(recognizer, stream);
        console.log('‚úÖ Audio decoded');
        
        // Get the transcription result
        const resultJson = sherpa.getOfflineStreamResultAsJson(stream);
        const result = JSON.parse(resultJson);
        
        console.log('‚úÖ Transcription completed');
        console.log();
        
        return result;
        
    } catch (error) {
        console.error('‚ùå Transcription failed:', error.message);
        return null;
    }
}

function displayResults(result) {
    console.log('üìä TRANSCRIPTION RESULTS:');
    console.log('========================');
    
    if (result && result.text && result.text.trim()) {
        console.log('üéâ SUCCESS! Transcribed text:');
        console.log(`   "${result.text}"`);
        console.log();
        
        if (result.words && result.words.length > 0) {
            console.log('üìù Word-by-word breakdown:');
            result.words.forEach((word, index) => {
                console.log(`   ${index + 1}. "${word.word}" (confidence: ${word.confidence || 'N/A'})`);
            });
            console.log();
        }
        
        if (result.timestamps && result.timestamps.length > 0) {
            console.log('‚è∞ Timestamps:');
            result.timestamps.forEach((timestamp, index) => {
                console.log(`   ${index + 1}. ${timestamp}s`);
            });
            console.log();
        }
        
    } else {
        console.log('‚ö†Ô∏è  No speech detected in the audio');
        console.log('   This is expected for synthetic test audio.');
        console.log('   With real microphone input, you would see transcribed text here.');
        console.log();
    }
    
    console.log('üîç Raw result object:');
    console.log(JSON.stringify(result, null, 2));
}

async function main() {
    try {
        console.log('üöÄ Starting Live Microphone Test...');
        console.log();
        
        // Step 1: Load native module
        await loadNativeModule();
        
        // Step 2: Setup recognizer
        const setupSuccess = await setupRecognizer();
        if (!setupSuccess) {
            console.error('üí• Failed to setup recognizer');
            process.exit(1);
        }
        
        console.log();
        console.log('üé§ READY FOR SPEECH INPUT!');
        console.log('==========================');
        console.log();
        
        // Step 3: Simulate microphone input (in real version, this would be live audio)
        const audioData = await simulateMicrophoneInput();
        
        // Step 4: Transcribe the audio
        const result = await transcribeAudio(audioData);
        
        // Step 5: Display results
        displayResults(result);
        
        console.log();
        console.log('üí° TO USE WITH REAL MICROPHONE:');
        console.log('===============================');
        console.log('1. Replace simulateMicrophoneInput() with real microphone capture');
        console.log('2. Use Web Audio API or node-microphone library');
        console.log('3. Stream audio chunks to the transcriber in real-time');
        console.log('4. Display transcription results as they come in');
        console.log();
        console.log('üéØ The core transcription engine is working perfectly!');
        console.log('   Just needs real microphone integration.');
        
    } catch (error) {
        console.error('üí• Test failed:', error.message);
        console.error('Stack trace:', error.stack);
        process.exit(1);
    }
}

// Handle Ctrl+C gracefully
process.on('SIGINT', () => {
    console.log('\n\nüõë Stopping transcription...');
    console.log('üëã Thanks for testing the speech-to-text system!');
    process.exit(0);
});

// Run the live test
console.log('Press Ctrl+C to stop at any time');
console.log();
main().catch(console.error);
